# Pipeline de Previs√£o do Tempo com Apache Airflow

## üí° Resumo do projeto

Esse projeto tem como objetivo desenvolver um pipeline automatizado em Apache Airflow para coletar dados meteorol√≥gicos de m√∫ltiplas cidades via API p√∫blica, transformar as informa√ß√µes relevantes e armazen√°-las em um banco SQLite para an√°lises hist√≥ricas e monitoramento clim√°tico.

## ‚ùì Problema de neg√≥cio / contexto

Organiza√ß√µes que dependem de dados clim√°ticos (log√≠stica, energia, agroneg√≥cio, planejamento urbano, entre outras) precisam de informa√ß√µes atualizadas e estruturadas sobre o clima em diferentes regi√µes. A coleta manual ou espor√°dica desses dados dificulta an√°lises hist√≥ricas, compara√ß√µes e automa√ß√µes.

Desse modo, este projeto resolve essa dor ao criar uma rotina automatizada e confi√°vel que coleta, processa e armazena dados de previs√£o do tempo de forma recorrente, garantindo disponibilidade cont√≠nua das informa√ß√µes para an√°lises e tomada de decis√£o.

## üìä Dados utilizados

* Fonte dos dados: API p√∫blica do OpenWeatherMap
* Tipo de dados: Dados clim√°ticos em (quase) tempo real
* Cidades monitoradas:
    * Indaiatuba, Blumenau, Palmas, Joinville, Santos, Curitiba, Fortaleza, Manaus, Betim e Juazeiro
* Principais atributos coletados:
    * Nome da cidade
    * Data da medi√ß√£o
    * Temperatura (em graus Celsius)
    * Descri√ß√£o do clima (Ex.: c√©u limpo, nublado, chuva)
* Tratamentos aplicados:
    * Convers√£o de temperatura de Kelvin para Celsius
    * Padroniza√ß√£o da data
    * Sele√ß√£o apenas dos campos relevantes

## üõ†Ô∏è Metodologia e ferramentas

**Metodologia (ETL)**

1. Extra√ß√£o:
    * Consumo da API do OpenWeatherMap via requisi√ß√µes HTTP
    * Coleta dos dados brutos em formato JSON para cada cidade

2. Transforma√ß√£o:
    * Sele√ß√£o das informa√ß√µes relevantes
    * Convers√£o de temperatura
    * Estrutura√ß√£o dos dados em formato tabular

3. Carga:
    * Cria√ß√£o autom√°tica da tabela e do banco SQLite, caso n√£o exista
    * Ingest√£o dos dados na tabela

**Ferramentas e tecnologias**

* Apache Airflow: Orquestra√ß√£o e agendamento do pipeline
* Python: Linguagem principal do projeto
* SQLite: Banco de dados leve para armazenamento local
* Bibliotecas Python:
    * ```requests``` -> Consumo da API
    * ```sqlite3``` ‚Äì> Intera√ß√£o com banco de dados
    * ```datetime``` ‚Äì> Manipula√ß√£o de datas
* Airflow Operators:
    * ```PythonOperator``` -> execu√ß√£o das fun√ß√µes de ETL

## üìà Principais insights e resultados

* Cria√ß√£o de uma base hist√≥rica de dados clim√°ticos por cidade
* Automa√ß√£o completa do processo, eliminando interven√ß√£o manual
* Estrutura pronta para:
    * An√°lises de varia√ß√£o de temperatura
    * Monitoramento clim√°tico por regi√£o
    * Integra√ß√£o futura com dashboards ou modelos anal√≠ticos
* Pipeline facilmente escal√°vel para inclus√£o de novas cidades ou novos atributos clim√°ticos

**Valor gerado**

Disponibiliza√ß√£o cont√≠nua de dados confi√°veis e estruturados, reduzindo esfor√ßo operacional e aumentando a capacidade anal√≠tica da organiza√ß√£o.

## üöÄ Como executar o projeto

**Pr√©-requisitos**

* Docker e Docker Compose (recomendado para Airflow)
* Python 3.8+
* Apache Airflow configurado
* Acesso √† API do OpenWeatherMap (chave de API v√°lida)

**Pr√©-requisitos**

1. Clonar o reposit√≥rio

```git clone https://github.com/glaubermateus/Engenharia_Dados.git```

```cd seu-repositorio```

2. Configurar o Airflow usando o Docker Desktop

* Instale o Docker Desktop

* Crie um diret√≥rio para seus arquivos do Airflow

* Navegue at√© ele

* Baixe o arquivo docker-compose.yaml da documenta√ß√£o oficial do Airflow (procure por "Docker Compose")

* Execute o comando para criar as imagens Docker do Airflow. Isso pode levar alguns minutos.

    ```docker compose up airflow-init```

* Execute o comando para inicializar o airflow

    ```docker compose up```

* Acesse o Airflow em http://localhost:8080. Use admin/admin como usu√°rio/senha iniciais (ou as credenciais definidas no .env se usar a instala√ß√£o padr√£o).

* Criar a seguinte estrutura de pastas:
    * config
    * dags
    * logs
    * plugins

* Coloque o arquivo Python da DAG dentro do diret√≥rio dags/ do Airflow
* Ajuste o caminho do banco SQLite, se necess√°rio

3. Executar o projeto

* Acesse a interface web do Airflow
* Ative a DAG chamada projeto
* O pipeline ser√° executado automaticamente conforme o agendamento (* * * * *, a cada minuto)

4. Visualizar os dados

* O banco **'banco_dados.db'** ser√° criado automaticamente
* A tabela **'previsao_tempo'** conter√° os dados processados

## ü§ù Contato

Glauber Cruz

[LinkedIn](https://www.linkedin.com/in/glauber-cruz-6213281b0/)

[Portf√≥lio](https://sites.google.com/view/glaubercruz/p%C3%A1gina-inicial)
